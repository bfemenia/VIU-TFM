#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jun 21 12:35:06 2020

@author: bruno.femenia@gmail.com

This code takes the cvs files produced during the MapRed process with Hadoop 
and reformats the data within the cvs file into the DataFrames expected by the 
post-processing routines generated in RL19.
"""

# %%   IMPORT SECTION

import pandas as pd
import numpy  as np
import datetime
import matplotlib.pyplot as plt

from sklearn.metrics.pairwise import pairwise_distances
from sklearn.cluster import DBSCAN

from reducerTFM import *   #Recall DiasCatalog and GaiaData classes were
                           #defined within reducerTFM.py


# %%
#     Wrapper to match structure of O/P from MapRed to format in mainBL20.py
 
def results_to_df(ipfile= 'results_DC_l00_l05.csv',
                  ipath = '/home/TFM/Results_DC/', 
                  file_to_extract= 'ip_file_DC_00748.csv'):
    '''
    Bruno Femenia Castella
    
    This routine loads into a Pandas DataFrame the results of the execution
    of the code in MapRed in Hadoop. This acts as a wrapper between what
    reducerTFM.py produces and what the rest of post-processing and plotting
    routines in RL19 are expecting

    Parameters
    ----------
    ipfile : string, 
        The overall file storing ALL the part-00* files generated by MapRed
        during the bath processing.
        The default is 'results_DC_l00_l05.csv'
        
    ipath : string,
        Path to where ip file is. The default is '/home/TFM/Results_DC/'.
        
    file_to_extract : string,
        specifies which file in the batch process is to be processed.
        The default is 'ip_file_DC_00748.csv'.

    Returns
    -------
    df    : Pandas DataFrame,
            df follows same structure as in original RL19 and BF20 codes.

    '''
    
    
    #Reading full file but only keeping the rows with file_to_extract
    #Also reading a varaible number of columns. 
    #Solutions obtained from:
    #    
    # stackoverflow.com/questions/13651117/how-can-i-filter-lines-on-load-in-
    #pandas-read-csv-function
    #
    # stackoverflow.com/questions/15242746/handling-variable-number-of-columns
    #-with-pandas-python
 
    c1 = ['ipfile', 'ra', 'dec', 'l', 'b', 'r', 'glim', 'sample',
          'epsilon', 'minpts', 'local_score']
    c2 = []
    for i in range(3000): #10000
        dummy = 'clstr_' + str(i).zfill(4)
        c2.append(dummy+'_ra')
        c2.append(dummy+'_dec')
    
    c_names  = c1 + c2  
    iter_csv = pd.read_csv(ipath+ipfile,  iterator=True, chunksize=9100, 
                           names=c_names, engine='python')
    
    df_tmp  = pd.concat([chunk[chunk['ipfile'] == file_to_extract] 
                         for chunk in iter_csv])
    df_tmp.reset_index(drop=True, inplace=True)
    
    sscores      = df_tmp[['epsilon', 'minpts', 'local_score']]
    sscores_meta = df_tmp.iloc[0][c1[0:8]]
    
    return(sscores, sscores_meta)


#sscores, sscores_meta = results_to_df('results_test.csv') 


# %% DATA PLOTTING: taken directly from nain_BF20.py
def plot_clusters(X, labels, op_name):
    # Black removed and is used for noise instead.
    size = 6.0
    f = plt.figure(figsize=(20,20))
    unique_labels = set(labels)
    colors = [plt.cm.Spectral(each)
              for each in np.linspace(0, 0.5, len(unique_labels))]
    for k, col in zip(unique_labels, colors):
        if k == -1:
            # Black used for noise.
            col = [0, 0, 0, 1]
            size = 3.5

        class_member_mask = (labels == k)

        xy = X[class_member_mask]
        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
                 markeredgecolor='k', markersize=size)
        size=6.0

    plt.title('Estimated number of clusters: %5d' % int(len(set(labels))-1))
    plt.xlabel("Right Ascension (deg)")
    plt.ylabel("Declination (deg)")
    
    f.savefig(op_name)

    
# %%    
def plot_score(param_scores, op_name):
    np_param_scores = param_scores.values
    eps = np.sort(np.array(list(set(param_scores['epsilon']))))
    Nmin = np.sort(np.array(list(set(param_scores['minpts']))))
    Z = np.empty((len(Nmin), len(eps)))
    fig, ax = plt.subplots(constrained_layout = True)
    X, Y = np.meshgrid(eps, Nmin)
    for i, n in enumerate(Nmin):
        for j, e in enumerate(eps):
            Z[i,j] = np_param_scores[np.where((param_scores['epsilon'] == e) & 
                                              (param_scores['minpts'] == n)),2]
    extend = "neither"
    cmap = plt.cm.get_cmap('hot')
    CS = ax.contourf(X,Y,Z, cmap=cmap, extend=extend)
    fig.colorbar(CS)
    ax.set_xlabel('Epsilon')
    ax.set_ylabel('Nmin')
    ax.set_title('DBSCAN matching M')
    
    fig.savefig(op_name)


# %% 
#    Using the above as a wrapper between MapRed O/P and expected by BL20
#    this resumes where reducerTFM left it at.

def process_reducerTFM(ipfile ='results_DC_l00_l05.csv',
                       ipath ='/home/TFM/Results_DC/', plot=False,
                       file_to_extract='ip_file_DC_00748.csv'):
    '''
    Bruno Femenia Castella

    Using results_to_df as a WRAPPER to put the MapReduce O/P data in the
    format expected by BL20, this routine resumes BL20 from where reducerTFM
    left it at.

    Parameters
    ----------
    ipfile : string, 
        The overall file storing ALL the part-00* files generated by MapRed
        during the bath processing.
        The default is 'results_DC_l00_l05.csv'
        
    ipath : string,
        Path to where ip file is. The default is '/home/TFM/Results_DC/'.
        
    file_to_extract : string,
        specifies which file in the batch process is to be processed.
        The default is 'ip_file_DC_00748.csv'.

    Returns
    -------
    None.

    '''
    
    # 1/ Loading results from MapRed for this ip_file_DC
    #---------------------------------------------------
    param_scores, param_meta = results_to_df(ipfile, ipath, file_to_extract)
    
    
    # 2/ Launching the Gaia query to retrieve the data in this ip_file_DC
    #    This is entirely for visual purposes displaying the stars and
    #    and clusters detected in each field sampled by ip_file_DC
    #--------------------------------------------------------------------
    coord_icrs =[ param_meta['ra'], param_meta['dec'] ]  # RA & Dec
    coord_gal  =[ param_meta['l' ], param_meta['b' ]  ]  # l  & b
    radius     =  param_meta['r' ]                       # radius
    g_lim      =  param_meta['glim'  ]                   # g_lim
    sample     =  param_meta['sample']
    
    data, center  = extract_data(radius, 100, g_lim, 
                                 coord_icrs=coord_icrs, coord_gal=coord_gal)
    
    
    # 3/ Last to be at where reducerTFM left wrt BF20 -> we need dist_matrix
    #------------------------------------------------------------------------
    data_sphe, data_cart, data_all = preprocessing(data, sample)
    data_search = data_sphe[:,:2]                #For TFM no scale -> no NORM.
    dm = pairwise_distances(data_search,         #For TFM always ftype='sphe'
                            metric='euclidean',  #Metric always euclidean
                            n_jobs=-1)
    dist_matrix = dm
    
    
    # 4/ Running same sequence of steps as in DBSCAN_result
    #------------------------------------------------------
    dias_catalog = DiasCatalog('Cluster','RAJ2000','DEJ2000','l', 'b','Class',
                               'Diam','Dist','pmRA','pmDE','Nc', 'RV', 'o_RV',
                               l0=coord_gal[0], b0=coord_gal[1], radius=radius)
        
    cluster_centers = []
    sorted_scores= param_scores.sort_values(by=['local_score'],ascending=False)
    opt_epsilon  = sorted_scores.iloc[0,0]
    opt_min_pts  = sorted_scores.iloc[0,1]
    best_M       = sorted_scores.iloc[0,2]
    
    
    # 5/ Launching again DBSCAN with optimal pars. This to retrieve the 
    #    detected clusters. We ca do this faster but it takes shorter to 
    #    copy the lines (to improve: this is already in ip_file)
    #--------------------------------------------------------------------
    id_str = ((file_to_extract.split('_')[3]).split('.'))[0] 
    id_nb  = int(id_str)
    ra     = coord_icrs[0]
    dec    = coord_icrs[1]
    l      = coord_gal[ 0]
    b      = coord_gal[ 1]

    if (best_M > 0) and plot:
        db = DBSCAN(eps=opt_epsilon, min_samples=opt_min_pts, 
                    metric='precomputed', n_jobs=-1).fit(dist_matrix)
    
        labels = db.labels_
        for i in range(len(set(labels))-1):
            cluster = data[np.where(labels == i)]
            cluster_center = cluster.mean(axis=0)
            cluster_centers.append(cluster_center)
        
        matches = dias_catalog.get_match(center, radius, cluster_centers)
        for i, m in enumerate(matches):
            if (i==0): 
                print(f'\n\nResults for {file_to_extract}:\n'+33*'=')
                print(f'\tBox (RA, Dec) = ({ra}. {dec}) degs , '
                      f'side={2*radius} degs\n')
            
            print('\t\t>> Cluster found: %s'%(m[0]))
        
        plot_clusters(data, labels, 'plt_ID'+id_str+'_clstrs.png')
        plot_score(param_scores,    'plt_ID'+id_str+'_scores.png')
    
    return(id_nb, ra, dec, l, b, opt_epsilon, opt_min_pts, best_M)


# %%
#     Creating maps of opt_epsilon, opt_min_pts, best_M

def get_df(ipfile ='results_DC_l00_l05.csv',
           ipath ='/home/TFM/Results_DC/',
           batch ='batch_DC_l_00_l_05.txt'):
    
    cols = ['id', 'ra', 'dec', 'l', 'b', 'eps', 'n_pts', 'M']
    df   = pd.DataFrame(columns=cols)

    with open(ipath+batch,'r') as f:
        files = f.read().splitlines()

    for file in files:
        id_nb, ra, dec, l, b, opt_epsilon, opt_min_pts, best_M = \
           process_reducerTFM(ipfile=ipfile,ipath=ipath,file_to_extract=file)

        tmp = pd.Series([id_nb,ra,dec,l,b, opt_epsilon, opt_min_pts, best_M],
                        index=df.columns)
    
        df  = df.append(tmp, ignore_index=True)
    
    return(df)
    

ipath  = '/home/TFM/Results_DC/'
 
ipfile     = 'results_DC_l00_l05.csv'    #Analysing files l=[0,5]
batch      = 'batch_DC_l_00_l_05.txt'
df_l00_l05 = get_df(ipfile=ipfile, ipath=ipath, batch=batch)

ipfile = 'results_DC_l05_l14.csv'        #Analysing files l=[5,14]
batch  = 'batch_DC_l_05_l_14.txt'
df_l05_l14 = get_df(ipfile=ipfile, ipath=ipath, batch=batch)


